# Chapter 3. Named Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi Decoding

## Named Entity Recognition (NER)

Given a sentence or a piece of text, the objective of an NER model is to locate and classify text tokens as named entities in categories. 

## Load data

Using Groningen Meaning Bank (GMB) dataset [[link]](https://gmb.let.rug.nl/releases/gmb-2.2.0.zip)

For example: B-per, I-per, O, B-geo. So, TWO kinds of tags in the data. 

- ner_tags
- iob_tags

```
import csv
import collections

ner_tags = collections.Counter()
iob_tags = collections.Counter()

def strip_ner_subcat(tag):
    # NER tags are of form {cat}-{subcat}
    # eg tim-dow. We only want first part
    return tag.split("-")[0]

def iob_format(ners):
    # converts IO tags into IOB format
    # input is a sequence of IO NER tokens
    # convert this: O, PERSON, PERSON, O, O, LOCATION, O
    # into: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O
    iob_tokens = []
    for idx, token in enumerate(ners):
        if token != 'O': # !other
            if idx == 0:
                token = "B-" + token #start of sentence
            elif ners[idx-1] == token:
                token = "I-" + token # continues
            else:
                token = "B-" + token
        iob_tokens.append(token)
        iob_tags[token] += 1
    return iob_tokens

total_sentences = 0
outfiles = []
for idx, file in enumerate(fnames):
    with open(file, "rb") as content:
        data = content.read().decode("utf-8").strip()
        sentences = data.split("\n\n")
        print(idx, file, len(sentences))
        total_sentences += len(sentences)

        with open("./ner/"+str(idx)+"-"+os.path.basename(file),"w") as outfile:
            outfiles.append("./ner/"+str(idx)+"-"+os.path.basename(file))
            writer = csv.writer(outfile)

            for sentence in sentences:
                toks = sentence.split("\n")
                words, pos, ner = [], [], []
                for tok in toks:
                    t = tok.split("\t")
                    words.append(t[0])
                    pos.append(t[1])
                    ner_tags[t[3]] += 1
                    ner.append(strip_ner_subcat(t[3]))
                writer.writerow([" ".join(words), " ".join(iob_format(ner)), " ".join(pos)])
```



## **Conditional random fields**

From one condition to another, which can be presented by a matrix with indicative transition weights. 

Three main steps:
- Modifying the score generated by the BiLSTM layer, accounting for the transition weights.


## **NER with BiLSTM and CRFs**

Using 
```
!pip install tensorflow_addons==0.11.2
```

Low-level methods for implementing the CRF layer are provided. 

Implementing a custom layer in Keras requires subclassing **keras.layers.Layer**. The main method to be implemented is **call()**. 

### *Parameters*: 
- label_size, the number of labels
- mask_id, for future configurability
- trans_params, the transition matrix, to be learned

> The CRF layer merely regurgitates the outputs during training time. The CRF layer is useful only during inference. 

### *For call()*
- the function is called during training or during inference
- with fit(), learning_phase()=True
- with predict(), learning_phase()=False

Then put CRF into the NER model. 


## **Viterbi decoding** forward pass

Decode the sentence in the CRF layer to get the sequences. 

The Viterbi algorithm is used to take the predictions for each word in the sequence and apply a maximization algorithm, so that the output sequence has the highest likelihood. 

Using 

viterbi, viterbi_score = *tfa.text.viterbi_decode(score, transition_params)* [[doc]](https://tensorflow.google.cn/addons/api_docs/python/tfa/text/viterbi_decode?hl=zh-cn)

*CRF vs. Dropout*

- Dropout, modify inputs only during training time, and merely passes all the inputs through when inference.
- CRF, pass inputs through during training, and transform inputs when inference.




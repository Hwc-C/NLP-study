{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Kaggle Address**\n[link](https://www.kaggle.com/bradbolliger/gmb-v220)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-17T08:24:58.412701Z","iopub.execute_input":"2021-06-17T08:24:58.413191Z","iopub.status.idle":"2021-06-17T08:25:04.798458Z","shell.execute_reply.started":"2021-06-17T08:24:58.413094Z","shell.execute_reply":"2021-06-17T08:25:04.797498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls ../input/gmb-v220/gmb-2.2.0/data","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:25:04.799754Z","iopub.execute_input":"2021-06-17T08:25:04.800079Z","iopub.status.idle":"2021-06-17T08:25:05.552554Z","shell.execute_reply.started":"2021-06-17T08:25:04.800022Z","shell.execute_reply":"2021-06-17T08:25:05.551487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\ndata_root = \"../input/gmb-v220/gmb-2.2.0/data\"\nfnames = []\nfor root, dirs, files in os.walk(data_root):\n    for filename in files:\n        if filename.endswith(\".tags\"):\n            fnames.append(os.path.join(root, filename))\nfnames[:2]","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:25:05.555025Z","iopub.execute_input":"2021-06-17T08:25:05.555495Z","iopub.status.idle":"2021-06-17T08:25:29.989479Z","shell.execute_reply.started":"2021-06-17T08:25:05.555444Z","shell.execute_reply":"2021-06-17T08:25:29.988735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir ner","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:25:29.99071Z","iopub.execute_input":"2021-06-17T08:25:29.991113Z","iopub.status.idle":"2021-06-17T08:25:30.716184Z","shell.execute_reply.started":"2021-06-17T08:25:29.991083Z","shell.execute_reply":"2021-06-17T08:25:30.715083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nimport collections\n\nner_tags = collections.Counter()\niob_tags = collections.Counter()\n\ndef strip_ner_subcat(tag):\n    # NER tags are of form {cat}-{subcat}\n    # eg tim-dow. We only want first part\n    return tag.split(\"-\")[0]\n\ndef iob_format(ners):\n    # converts IO tags into IOB format\n    # input is a sequence of IO NER tokens\n    # convert this: O, PERSON, PERSON, O, O, LOCATION, O\n    # into: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O\n    iob_tokens = []\n    for idx, token in enumerate(ners):\n        if token != 'O': # !other\n            if idx == 0:\n                token = \"B-\" + token #start of sentence\n            elif ners[idx-1] == token:\n                token = \"I-\" + token # continues\n            else:\n                token = \"B-\" + token\n        iob_tokens.append(token)\n        iob_tags[token] += 1\n    return iob_tokens\n\ntotal_sentences = 0\noutfiles = []\nfor idx, file in enumerate(fnames):\n    with open(file, \"rb\") as content:\n        data = content.read().decode(\"utf-8\").strip()\n        sentences = data.split(\"\\n\\n\")\n        print(idx, file, len(sentences))\n        total_sentences += len(sentences)\n\n        with open(\"./ner/\"+str(idx)+\"-\"+os.path.basename(file),\"w\") as outfile:\n            outfiles.append(\"./ner/\"+str(idx)+\"-\"+os.path.basename(file))\n            writer = csv.writer(outfile)\n\n            for sentence in sentences:\n                toks = sentence.split(\"\\n\")\n                words, pos, ner = [], [], []\n                for tok in toks:\n                    t = tok.split(\"\\t\")\n                    words.append(t[0])\n                    pos.append(t[1])\n                    ner_tags[t[3]] += 1\n                    ner.append(strip_ner_subcat(t[3]))\n                writer.writerow([\" \".join(words), \" \".join(iob_format(ner)), \" \".join(pos)])","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:25:30.718106Z","iopub.execute_input":"2021-06-17T08:25:30.718542Z","iopub.status.idle":"2021-06-17T08:26:13.019439Z","shell.execute_reply.started":"2021-06-17T08:25:30.718496Z","shell.execute_reply":"2021-06-17T08:26:13.018545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"total number of sentences: \", total_sentences)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:13.020655Z","iopub.execute_input":"2021-06-17T08:26:13.020942Z","iopub.status.idle":"2021-06-17T08:26:13.024966Z","shell.execute_reply.started":"2021-06-17T08:26:13.020912Z","shell.execute_reply":"2021-06-17T08:26:13.024266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(ner_tags)\nprint(iob_tags)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:13.025967Z","iopub.execute_input":"2021-06-17T08:26:13.026254Z","iopub.status.idle":"2021-06-17T08:26:13.04086Z","shell.execute_reply.started":"2021-06-17T08:26:13.026224Z","shell.execute_reply":"2021-06-17T08:26:13.039828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Normalizing and vectorizing**","metadata":{}},{"cell_type":"code","source":"import glob\nimport pandas as pd\n# could use `outfiles` param as well\nfiles = glob.glob(\"./ner/*.tags\")\ndata_pd = pd.concat([pd.read_csv(f, header=None, names=[\"text\", \"label\", \"pos\"]) \n                     for f in files], ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:13.044078Z","iopub.execute_input":"2021-06-17T08:26:13.044564Z","iopub.status.idle":"2021-06-17T08:26:30.900191Z","shell.execute_reply.started":"2021-06-17T08:26:13.044526Z","shell.execute_reply":"2021-06-17T08:26:30.89918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_pd.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:30.902191Z","iopub.execute_input":"2021-06-17T08:26:30.9025Z","iopub.status.idle":"2021-06-17T08:26:30.951624Z","shell.execute_reply.started":"2021-06-17T08:26:30.902469Z","shell.execute_reply":"2021-06-17T08:26:30.950617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_pd.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:30.952753Z","iopub.execute_input":"2021-06-17T08:26:30.953062Z","iopub.status.idle":"2021-06-17T08:26:30.969043Z","shell.execute_reply.started":"2021-06-17T08:26:30.953032Z","shell.execute_reply":"2021-06-17T08:26:30.968107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Keras tokenizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n# oov_token -> unknown, Out-Of-Vocab?\ntext_tok = Tokenizer(filters='[\\\\]^\\t\\n', lower=False,\n                     split=' ', oov_token='<OOV>')\n\npos_tok = Tokenizer(filters='\\t\\n', lower=False,\n                    split=' ', oov_token='<OOV>')\n\nner_tok = Tokenizer(filters='\\t\\n', lower=False,\n                    split=' ', oov_token='<OOV>')","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:30.970091Z","iopub.execute_input":"2021-06-17T08:26:30.970356Z","iopub.status.idle":"2021-06-17T08:26:30.975736Z","shell.execute_reply.started":"2021-06-17T08:26:30.970329Z","shell.execute_reply":"2021-06-17T08:26:30.974669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_tok.fit_on_texts(data_pd['text'])\npos_tok.fit_on_texts(data_pd['pos'])\nner_tok.fit_on_texts(data_pd['label'])","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:30.977209Z","iopub.execute_input":"2021-06-17T08:26:30.977552Z","iopub.status.idle":"2021-06-17T08:26:34.327448Z","shell.execute_reply.started":"2021-06-17T08:26:30.977515Z","shell.execute_reply":"2021-06-17T08:26:34.326404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner_config = ner_tok.get_config()\ntext_config = text_tok.get_config()\nprint(\"ner_config:\\n\", ner_config)\n# print(\"text_config:\\n\", text_config)\n# print(\"pos_config:\\n\", pos_config)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:34.328957Z","iopub.execute_input":"2021-06-17T08:26:34.329287Z","iopub.status.idle":"2021-06-17T08:26:34.6114Z","shell.execute_reply.started":"2021-06-17T08:26:34.329255Z","shell.execute_reply":"2021-06-17T08:26:34.610439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# index to word\ntext_vocab = eval(text_config['index_word'])\nner_vocab = eval(ner_config['index_word'])\n\nprint(\"Unique words in vocab:\", len(text_vocab))\nprint(\"Unique NER tags in vocab:\", len(ner_vocab))","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:34.612655Z","iopub.execute_input":"2021-06-17T08:26:34.612982Z","iopub.status.idle":"2021-06-17T08:26:34.819015Z","shell.execute_reply.started":"2021-06-17T08:26:34.61295Z","shell.execute_reply":"2021-06-17T08:26:34.817985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_tok = text_tok.texts_to_sequences(data_pd['text'])\ny_tok = ner_tok.texts_to_sequences(data_pd['label'])","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:34.820172Z","iopub.execute_input":"2021-06-17T08:26:34.820443Z","iopub.status.idle":"2021-06-17T08:26:36.746811Z","shell.execute_reply.started":"2021-06-17T08:26:34.820415Z","shell.execute_reply":"2021-06-17T08:26:36.746041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import sequence\nmax_len = 50\nx_pad = sequence.pad_sequences(x_tok, padding='post', maxlen=max_len)\ny_pad = sequence.pad_sequences(y_tok, padding='post', maxlen=max_len)\nprint(x_pad.shape, y_pad.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:36.747801Z","iopub.execute_input":"2021-06-17T08:26:36.748233Z","iopub.status.idle":"2021-06-17T08:26:37.966575Z","shell.execute_reply.started":"2021-06-17T08:26:36.748202Z","shell.execute_reply":"2021-06-17T08:26:37.965916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = len(ner_vocab) + 1\n\n# One-Hot\nY = tf.keras.utils.to_categorical(y_pad, num_classes=num_classes)\nY.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:37.96752Z","iopub.execute_input":"2021-06-17T08:26:37.967905Z","iopub.status.idle":"2021-06-17T08:26:38.133559Z","shell.execute_reply.started":"2021-06-17T08:26:37.967859Z","shell.execute_reply":"2021-06-17T08:26:38.132539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **BiLSTM**","metadata":{}},{"cell_type":"code","source":"# Length of the vocabulary\nvocab_size = len(text_vocab) + 1\n\n# The embedding dimension\nembedding_dim = 64\n\n# Number of RNN units\nrnn_units = 100\n\n#batch size\nBATCH_SIZE=90\n\n# num of NER classes\nnum_classes = len(ner_vocab)+1","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:38.1346Z","iopub.execute_input":"2021-06-17T08:26:38.135002Z","iopub.status.idle":"2021-06-17T08:26:38.138745Z","shell.execute_reply.started":"2021-06-17T08:26:38.134972Z","shell.execute_reply":"2021-06-17T08:26:38.13814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dense\n\ndef build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size, classes):\n    model = tf.keras.Sequential([\n        Embedding(vocab_size, embedding_dim, mask_zero=True,\n                                  batch_input_shape=[batch_size, None]),\n        Bidirectional(LSTM(units=rnn_units, \n                               return_sequences=True, \n                               dropout=0.5,  \n                               kernel_initializer=tf.keras.initializers.he_normal())),\n        #  LSTM(rnn_units, return_sequences=True, \n        #           dropout=0.5, \n        #           recurrent_dropout=0.5),\n        TimeDistributed(Dense(rnn_units, activation=\"relu\")),\n        Dense(num_classes, activation=\"softmax\")\n#         tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:38.139682Z","iopub.execute_input":"2021-06-17T08:26:38.140058Z","iopub.status.idle":"2021-06-17T08:26:38.152464Z","shell.execute_reply.started":"2021-06-17T08:26:38.140029Z","shell.execute_reply":"2021-06-17T08:26:38.151563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model_bilstm(\n                        vocab_size = vocab_size,\n                        embedding_dim=embedding_dim,\n                        rnn_units=rnn_units,\n                        batch_size=BATCH_SIZE,\n                        classes=num_classes)\nmodel.summary()\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:38.153611Z","iopub.execute_input":"2021-06-17T08:26:38.153879Z","iopub.status.idle":"2021-06-17T08:26:39.702159Z","shell.execute_reply.started":"2021-06-17T08:26:38.153852Z","shell.execute_reply":"2021-06-17T08:26:39.701046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to enable TensorFlow to process sentences properly\nX = x_pad\n# create training and testing splits\ntotal_sentences = 62010\ntest_size = round(total_sentences / BATCH_SIZE * 0.2)\nX_train = X[BATCH_SIZE*test_size:]\nY_train = Y[BATCH_SIZE*test_size:]\n\nX_test = X[0:BATCH_SIZE*test_size]\nY_test = Y[0:BATCH_SIZE*test_size]","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:39.70372Z","iopub.execute_input":"2021-06-17T08:26:39.704164Z","iopub.status.idle":"2021-06-17T08:26:39.710118Z","shell.execute_reply.started":"2021-06-17T08:26:39.70412Z","shell.execute_reply":"2021-06-17T08:26:39.709138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=15)","metadata":{"execution":{"iopub.status.busy":"2021-06-17T08:26:39.711184Z","iopub.execute_input":"2021-06-17T08:26:39.71146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test, batch_size=BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_tok.sequences_to_texts([X_test[1]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ner_tok.sequences_to_texts([y_pad[1]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = tf.argmax(y_pred, -1)\ny_pred.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pnp = y_pred.numpy()\nner_tok.sequences_to_texts([y_pnp[1]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **BiLSTM-CRF**","metadata":{}},{"cell_type":"code","source":"import tensorflow_addons as tfa\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import backend as K","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CRFLayer(Layer):\n    \"\"\"\n    Computes the log likelihood during training\n    Performs Viterbi decoding during prediction\n    \"\"\"\n    def __init__(self,\n               label_size,\n               mask_id=0,\n               trans_params=None,\n               name='crf',\n               **kwargs):\n        super(CRFLayer, self).__init__(name=name, **kwargs)\n        self.label_size = label_size\n        self.mask_id = mask_id\n        self.transition_params = None\n\n        if trans_params is None:  # not reloading pretrained params\n            self.transition_params = tf.Variable(tf.random.uniform(shape=(label_size, label_size)),\n                                             trainable=False)\n        else:\n            self.transition_params = trans_params\n\n    def get_seq_lengths(self, matrix):\n        # matrix is of shape (batch_size, max_seq_len)\n        mask = tf.not_equal(matrix, self.mask_id)\n        seq_lengths = tf.math.reduce_sum(\n                                        tf.cast(mask, dtype=tf.int32), \n                                        axis=-1)\n        return seq_lengths\n\n    def call(self, inputs, seq_lengths, training=None):\n        if training is None:\n            training = K.learning_phase()\n\n        # during training, this layer just returns the logits\n        if training:\n            return inputs\n\n        # viterbi decode logic to return proper \n        # results at inference\n        _, max_seq_len, _ = inputs.shape\n        seqlens = seq_lengths\n        paths = []\n        for logit, text_len in zip(inputs, seqlens):\n            viterbi_path, _ = tfa.text.viterbi_decode(logit[:text_len], \n                                                  self.transition_params)\n            paths.append(self.pad_viterbi(viterbi_path, max_seq_len))\n\n        return tf.convert_to_tensor(paths) \n\n    def pad_viterbi(self, viterbi, max_seq_len):\n        if len(viterbi) < max_seq_len:\n            viterbi = viterbi + [self.mask_id] * (max_seq_len - len(viterbi))\n        return viterbi\n\n    def get_proper_labels(self, y_true):\n        shape = y_true.shape\n        if len(shape) > 2:\n            return tf.argmax(y_true, -1, output_type=tf.int32)\n        return y_true\n\n    def loss(self, y_true, y_pred):\n        y_pred = tf.convert_to_tensor(y_pred)\n        y_true = tf.cast(self.get_proper_labels(y_true), y_pred.dtype)\n\n        seq_lengths = self.get_seq_lengths(y_true)\n        log_likelihoods, self.transition_params = tfa.text.crf_log_likelihood(y_pred, \n                                                                    y_true, seq_lengths)\n        # save transition params\n        self.transition_params = tf.Variable(self.transition_params, trainable=False)\n        # calc loss\n        loss = - tf.reduce_mean(log_likelihoods)\n        return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Model, Input, Sequential\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed\nfrom tensorflow.keras.layers import Dropout, Bidirectional\nfrom tensorflow.keras import backend as K\n\nclass NerModel(tf.keras.Model):\n    def __init__(self, hidden_num, vocab_size, label_size, embedding_size,\n                name='BilstmCrfModel', **kwargs):\n        super(NerModel, self).__init__(name=name, **kwargs)\n        self.num_hidden = hidden_num\n        self.vocab_size = vocab_size\n        self.label_size = label_size\n\n        self.embedding = Embedding(vocab_size, embedding_size, \n                                   mask_zero=True, name=\"embedding\")\n        self.biLSTM =Bidirectional(LSTM(hidden_num, return_sequences=True), name=\"bilstm\")\n        self.dense = TimeDistributed(tf.keras.layers.Dense(label_size), name=\"dense\")\n        self.crf = CRFLayer(self.label_size, name=\"crf\")\n\n    def call(self, text, labels=None, training=None):\n        # zeros in padded sequence is None\n        seq_lengths = tf.math.reduce_sum(tf.cast(tf.math.not_equal(text, 0), \n                                               dtype=tf.int32), axis=-1) \n        \n        if training is None:\n            training = K.learning_phase()\n\n        inputs = self.embedding(text)\n        bilstm = self.biLSTM(inputs)\n        logits = self.dense(bilstm)\n        outputs = self.crf(logits, seq_lengths, training)\n        \n        return outputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Length of the vocabulary in chars\nvocab_size = len(text_vocab)+1 # len(chars)\n\n# The embedding dimension\nembedding_dim = 64\n\n# Number of RNN units\nrnn_units = 100\n\n#batch size\nBATCH_SIZE=90\n\n# num of NER classes\nnum_classes = len(ner_vocab)+1\n\nblc_model = NerModel(rnn_units, vocab_size, num_classes, embedding_dim, dynamic=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create training and testing splits\ntotal_sentences = 62010\ntest_size = round(total_sentences / BATCH_SIZE * 0.2)\nX_train = x_pad[BATCH_SIZE*test_size:]\nY_train = Y[BATCH_SIZE*test_size:]\n\nX_test = x_pad[0:BATCH_SIZE*test_size]\nY_test = Y[0:BATCH_SIZE*test_size]\nY_train_int = tf.cast(Y_train, dtype=tf.int32)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train_int))\ntrain_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_metric = tf.keras.metrics.Mean()\n\nepochs = 5\n\n# Iterate over epochs.\nfor epoch in range(epochs):\n    print('Start of epoch %d' % (epoch,))\n\n    # Iterate over the batches of the dataset.\n    for step, (text_batch, labels_batch) in enumerate(train_dataset):\n        labels_max = tf.argmax(labels_batch, -1, output_type=tf.int32)\n        with tf.GradientTape() as tape:\n            logits = blc_model(text_batch, training=True)\n            # the custom loss\n            loss = blc_model.crf.loss(labels_max, logits)\n\n            grads = tape.gradient(loss, blc_model.trainable_weights)\n            optimizer.apply_gradients(zip(grads, blc_model.trainable_weights))\n            \n            loss_metric(loss)\n        if step % 50 == 0:\n            print('step %s: mean loss = %s' % (step, loss_metric.result()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_test_int = tf.cast(Y_test, dtype=tf.int32)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test_int))\ntest_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n\nout = blc_model.predict(test_dataset.take(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_tok.sequences_to_texts([X_test[2]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Ground Truth: \",\nner_tok.sequences_to_texts([tf.argmax(Y_test[2],-1).numpy()]))\nprint(\"Prediction: \", ner_tok.sequences_to_texts([out[2]]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def np_precision(pred, true):\n    # expect numpy arrays\n    assert pred.shape == true.shape\n    assert len(pred.shape) == 2\n    mask_pred = np.ma.masked_equal(pred, 0)\n    mask_true = np.ma.masked_equal(true, 0)\n    acc = np.equal(mask_pred, mask_true)\n    return np.mean(acc.compressed().astype(int))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np_precision(out, tf.argmax(Y_test[:BATCH_SIZE], -1).numpy())","metadata":{},"execution_count":null,"outputs":[]}]}